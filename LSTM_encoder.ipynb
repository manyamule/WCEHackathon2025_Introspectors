{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyECZRAP9q8+p95H9aQ59X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manyamule/WCEHackathon2025_Introspectors/blob/main/LSTM_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKEyNFf7RNDu",
        "outputId": "5fe45df6-04b6-4732-c530-c23bea1651f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "AgdyNAc8RBju",
        "outputId": "e17221ab-05ee-476c-d515-af1f578996ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:LSTMAnomalyDetection:Dataset contains 1146528 missing values.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 0.8719 - val_loss: 0.1922 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.5171 - val_loss: 0.1606 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.4542 - val_loss: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3885 - val_loss: 0.1170 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3616 - val_loss: 0.1080 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3533 - val_loss: 0.1036 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3305 - val_loss: 0.1111 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3422 - val_loss: 0.1042 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3317 - val_loss: 0.1001 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3257 - val_loss: 0.0974 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3214 - val_loss: 0.0944 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.3097 - val_loss: 0.0905 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.3056 - val_loss: 0.0976 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3106 - val_loss: 0.0987 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3038 - val_loss: 0.1051 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2899 - val_loss: 0.0930 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2881 - val_loss: 0.0947 - learning_rate: 5.0000e-04\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import os\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "# Check for GPU availability and log it\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"GPUs available:\", gpus)\n",
        "else:\n",
        "    print(\"No GPU available. Running on CPU.\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"lstm_anomaly_detection.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"LSTMAnomalyDetection\")\n",
        "\n",
        "\n",
        "class LSTMAnomalyDetector:\n",
        "    \"\"\"\n",
        "    LSTM Autoencoder-based anomaly detection for time series data with improvements:\n",
        "    - Time-based train-test split\n",
        "    - Dynamic threshold selection using training reconstruction error distribution\n",
        "    - Additional callbacks for faster convergence using GPU\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize with configuration parameters.\n",
        "        Creates directories for saving models, plots, and reports.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.data = None\n",
        "        self.preprocessed_data = None\n",
        "        self.train_data = None\n",
        "        self.test_data = None\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.threshold = None\n",
        "\n",
        "        # Create output directories if they don't exist\n",
        "        os.makedirs(config['model_dir'], exist_ok=True)\n",
        "        os.makedirs(config['plot_dir'], exist_ok=True)\n",
        "        os.makedirs(config['report_dir'], exist_ok=True)\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        \"\"\"\n",
        "        Load data from a CSV file and set the datetime index.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading data from CSV: {csv_path}\")\n",
        "        try:\n",
        "            self.data = pd.read_csv(csv_path)\n",
        "            if 'dt_time' in self.data.columns:\n",
        "                self.data['dt_time'] = pd.to_datetime(self.data['dt_time'])\n",
        "                self.data.set_index('dt_time', inplace=True)\n",
        "            logger.info(f\"Data loaded. Shape: {self.data.shape}\")\n",
        "            logger.info(f\"Columns: {self.data.columns.tolist()}\")\n",
        "            logger.info(f\"Date range: {self.data.index.min()} to {self.data.index.max()}\")\n",
        "            missing_values = self.data.isnull().sum().sum()\n",
        "            if missing_values > 0:\n",
        "                logger.warning(f\"Dataset contains {missing_values} missing values.\")\n",
        "            return self.data\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Preprocess the data:\n",
        "        - Select specific parameters if provided\n",
        "        - Handle missing values (forward/backward fill)\n",
        "        - Remove duplicate timestamps\n",
        "        - Resample to regular intervals if required\n",
        "        - Perform a time-based train-test split\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting data preprocessing\")\n",
        "        if self.data is None:\n",
        "            logger.error(\"No data loaded. Please load data first.\")\n",
        "            return\n",
        "        df = self.data.copy()\n",
        "\n",
        "        # Select parameters\n",
        "        params = self.config.get('params', [])\n",
        "        if params and all(param in df.columns for param in params):\n",
        "            logger.info(f\"Selecting parameters: {params}\")\n",
        "            df = df[params]\n",
        "        elif params:\n",
        "            available_params = [p for p in params if p in df.columns]\n",
        "            logger.warning(f\"Some requested parameters not found. Using available: {available_params}\")\n",
        "            df = df[available_params]\n",
        "\n",
        "        # Handle missing values\n",
        "        if df.isnull().sum().sum() > 0:\n",
        "            logger.info(f\"Handling {df.isnull().sum().sum()} missing values\")\n",
        "            df.ffill(inplace=True)\n",
        "            df.bfill(inplace=True)\n",
        "\n",
        "        # Remove duplicate timestamps\n",
        "        if df.index.duplicated().any():\n",
        "            duplicates = df.index.duplicated().sum()\n",
        "            logger.info(f\"Removing {duplicates} duplicate timestamps\")\n",
        "            df = df[~df.index.duplicated()]\n",
        "\n",
        "        # Resample data if needed\n",
        "        if self.config.get('resample', False):\n",
        "            freq = self.config.get('resample_freq', '1h')\n",
        "            logger.info(f\"Resampling data to {freq}\")\n",
        "            df = df.resample(freq).mean()\n",
        "            df.ffill(inplace=True)\n",
        "            df.bfill(inplace=True)\n",
        "\n",
        "        self.preprocessed_data = df\n",
        "        logger.info(f\"Preprocessing complete. Shape: {df.shape}\")\n",
        "\n",
        "        # Perform time-based train-test split (default 80/20)\n",
        "        train_ratio = self.config.get('train_ratio', 0.8)\n",
        "        split_index = int(len(df) * train_ratio)\n",
        "        self.train_data = df.iloc[:split_index]\n",
        "        self.test_data = df.iloc[split_index:]\n",
        "        logger.info(f\"Train data shape: {self.train_data.shape}, Test data shape: {self.test_data.shape}\")\n",
        "        return df\n",
        "\n",
        "    def _create_sequences(self, data, seq_length):\n",
        "        \"\"\"\n",
        "        Create sequences for LSTM input from the scaled data.\n",
        "        Each sequence is of length 'seq_length'.\n",
        "        \"\"\"\n",
        "        X = []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            X.append(data[i:i + seq_length])\n",
        "        return np.array(X)\n",
        "\n",
        "    def _build_lstm_autoencoder(self, input_shape):\n",
        "        \"\"\"\n",
        "        Build and compile the LSTM autoencoder model.\n",
        "        \"\"\"\n",
        "        encoding_dim = self.config.get('encoding_dim', 32)\n",
        "        dropout_rate = self.config.get('dropout_rate', 0.2)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(input_shape[1], input_shape[2])))\n",
        "        # Encoder\n",
        "        model.add(LSTM(units=encoding_dim * 2, activation='relu', return_sequences=True))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(LSTM(units=encoding_dim, activation='relu', return_sequences=False))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        # Bottleneck\n",
        "        model.add(Dense(encoding_dim // 2, activation='relu'))\n",
        "        # Decoder\n",
        "        model.add(Dense(encoding_dim, activation='relu'))\n",
        "        model.add(Dense(input_shape[1] * input_shape[2], activation='linear'))\n",
        "        model.add(Reshape((input_shape[1], input_shape[2])))\n",
        "\n",
        "        model.compile(optimizer=self.config.get('optimizer', 'adam'), loss=self.config.get('loss', 'mse'))\n",
        "        model.summary(print_fn=logger.info)\n",
        "        return model\n",
        "\n",
        "    def train_lstm_autoencoder(self):\n",
        "        \"\"\"\n",
        "        Train the LSTM autoencoder using the training data.\n",
        "        Also, compute the reconstruction error on the training set to set a dynamic anomaly threshold.\n",
        "        \"\"\"\n",
        "        logger.info(\"Training LSTM Autoencoder model\")\n",
        "        if self.train_data is None:\n",
        "            logger.error(\"No training data available. Run preprocess_data first.\")\n",
        "            return\n",
        "\n",
        "        df_train = self.train_data.copy()\n",
        "\n",
        "        # Scale training data\n",
        "        self.scaler = StandardScaler()\n",
        "        scaled_train = self.scaler.fit_transform(df_train)\n",
        "\n",
        "        seq_length = self.config.get('lstm_sequence_length', 24)\n",
        "        X_train = self._create_sequences(scaled_train, seq_length)\n",
        "        logger.info(f\"Created {len(X_train)} training sequences with length {seq_length}\")\n",
        "\n",
        "        self.model = self._build_lstm_autoencoder(X_train.shape)\n",
        "\n",
        "        # Define callbacks: early stopping and learning rate reduction for faster convergence\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=self.config.get('early_stopping_patience', 5), restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train, X_train,\n",
        "            epochs=self.config.get('lstm_epochs', 50),\n",
        "            batch_size=self.config.get('lstm_batch_size', 32),\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self._plot_training_history(history)\n",
        "\n",
        "        # Save the trained model and scaler\n",
        "        model_path = os.path.join(self.config['model_dir'], 'lstm_autoencoder_model.keras')\n",
        "        scaler_path = os.path.join(self.config['model_dir'], 'lstm_scaler.joblib')\n",
        "        self.model.save(model_path)\n",
        "        joblib.dump(self.scaler, scaler_path)\n",
        "        logger.info(f\"Model saved to {model_path}\")\n",
        "\n",
        "        # Compute reconstruction errors on training data to set anomaly threshold\n",
        "        reconstructions = self.model.predict(X_train)\n",
        "        mse = np.mean(np.square(X_train - reconstructions), axis=(1, 2))\n",
        "        factor = self.config.get('anomaly_threshold_factor', 3)  # e.g., 3 standard deviations\n",
        "        self.threshold = np.mean(mse) + factor * np.std(mse)\n",
        "        logger.info(f\"Anomaly threshold set to {self.threshold} using factor {factor}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def detect_anomalies(self, data=None):\n",
        "        \"\"\"\n",
        "        Detect anomalies on the provided data (or default to test data).\n",
        "        Uses the previously set dynamic threshold.\n",
        "        \"\"\"\n",
        "        logger.info(\"Detecting anomalies with LSTM Autoencoder\")\n",
        "        if data is not None:\n",
        "            df = data.copy()\n",
        "        elif self.test_data is not None:\n",
        "            df = self.test_data.copy()\n",
        "        else:\n",
        "            logger.error(\"No data available for anomaly detection\")\n",
        "            return None\n",
        "\n",
        "        # Scale the data using the saved scaler\n",
        "        scaled_data = self.scaler.transform(df)\n",
        "        seq_length = self.config.get('lstm_sequence_length', 24)\n",
        "        X = self._create_sequences(scaled_data, seq_length)\n",
        "        reconstructions = self.model.predict(X)\n",
        "        mse = np.mean(np.square(X - reconstructions), axis=(1, 2))\n",
        "\n",
        "        # Use the stored threshold; fallback to percentile method if not set\n",
        "        threshold = self.threshold if self.threshold is not None else np.percentile(mse, 100 - self.config.get('anomaly_percent', 1))\n",
        "        logger.info(f\"Using anomaly threshold: {threshold}\")\n",
        "\n",
        "        # Create a result DataFrame that aligns with the original timestamps (accounting for sequence length)\n",
        "        result = pd.DataFrame(index=df.index[seq_length:])\n",
        "        result['reconstruction_error'] = mse\n",
        "        result['anomaly'] = mse > threshold\n",
        "\n",
        "        # Include original data columns for reference\n",
        "        for col in df.columns:\n",
        "            result[col] = df[col].values[seq_length:]\n",
        "\n",
        "        anomaly_count = result['anomaly'].sum()\n",
        "        anomaly_percent = (anomaly_count / len(result)) * 100\n",
        "        logger.info(f\"Detected {anomaly_count} anomalies ({anomaly_percent:.2f}%)\")\n",
        "\n",
        "        self._plot_anomaly_results(result)\n",
        "\n",
        "        result_path = os.path.join(self.config['report_dir'], 'lstm_anomalies.csv')\n",
        "        result.to_csv(result_path)\n",
        "        logger.info(f\"Anomaly detection results saved to {result_path}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _plot_training_history(self, history):\n",
        "        \"\"\"\n",
        "        Plot and save the training and validation loss curves.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.semilogy(history.history['loss'], label='Training Loss')\n",
        "        plt.semilogy(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Loss (Log Scale)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Log Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.config['plot_dir'], 'lstm_training_history.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_anomaly_results(self, result_df):\n",
        "        \"\"\"\n",
        "        Plot the reconstruction error and overlay detected anomalies.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        # Plot reconstruction error over time\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(result_df.index, result_df['reconstruction_error'], label='Reconstruction Error')\n",
        "        plt.axhline(y=self.threshold, color='r', linestyle='--', label='Threshold')\n",
        "        plt.title('LSTM Autoencoder Reconstruction Error')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Error')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot original time series with anomalies highlighted\n",
        "        plt.subplot(2, 1, 2)\n",
        "        params = self.config.get('params', [col for col in result_df.columns if col not in ['reconstruction_error', 'anomaly']])\n",
        "        param = params[0] if params else result_df.columns[1]\n",
        "        plt.plot(result_df.index, result_df[param], label=param)\n",
        "        anomalies = result_df[result_df['anomaly']]\n",
        "        plt.scatter(anomalies.index, anomalies[param], color='red', label='Anomalies', s=50)\n",
        "        plt.title(f'LSTM Autoencoder Anomalies in {param}')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(param)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.config['plot_dir'], 'lstm_anomalies_overview.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def analyze_anomalies(self, anomaly_results):\n",
        "        \"\"\"\n",
        "        Analyze detected anomalies:\n",
        "        - Generate summary statistics (means, std, min/max)\n",
        "        - Analyze temporal patterns (by hour and day)\n",
        "        - Group consecutive anomalies into potential events\n",
        "        - Save the summary as a JSON report\n",
        "        \"\"\"\n",
        "        if anomaly_results is None or anomaly_results.empty:\n",
        "            logger.error(\"No anomaly results to analyze\")\n",
        "            return\n",
        "\n",
        "        logger.info(\"Analyzing anomalies\")\n",
        "        anomalies = anomaly_results[anomaly_results['anomaly']]\n",
        "        if anomalies.empty:\n",
        "            logger.info(\"No anomalies detected\")\n",
        "            return\n",
        "\n",
        "        params = [col for col in anomalies.columns if col not in ['reconstruction_error', 'anomaly']]\n",
        "        summary = {\n",
        "            'total_points': len(anomaly_results),\n",
        "            'anomaly_count': int(anomalies.shape[0]),\n",
        "            'anomaly_percent': float((anomalies.shape[0] / len(anomaly_results)) * 100),\n",
        "            'parameters': {}\n",
        "        }\n",
        "\n",
        "        # Compute statistics for each parameter\n",
        "        for param in params:\n",
        "            normal = anomaly_results[~anomaly_results['anomaly']][param]\n",
        "            anomaly_vals = anomalies[param]\n",
        "            summary['parameters'][param] = {\n",
        "                'normal_mean': float(normal.mean()),\n",
        "                'normal_std': float(normal.std()),\n",
        "                'anomaly_mean': float(anomaly_vals.mean()),\n",
        "                'anomaly_std': float(anomaly_vals.std()),\n",
        "                'anomaly_min': float(anomaly_vals.min()),\n",
        "                'anomaly_max': float(anomaly_vals.max()),\n",
        "                'stddev_distance': float(abs(anomaly_vals.mean() - normal.mean()) / normal.std() if normal.std() > 0 else 0)\n",
        "            }\n",
        "\n",
        "        # Analyze temporal patterns: by hour of day\n",
        "        if hasattr(anomalies.index, 'hour'):\n",
        "            hour_counts = anomalies.groupby(anomalies.index.hour).size()\n",
        "            summary['hour_distribution'] = hour_counts.to_dict()\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            hour_counts.plot(kind='bar')\n",
        "            plt.title('Distribution of Anomalies by Hour')\n",
        "            plt.xlabel('Hour')\n",
        "            plt.ylabel('Count')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.config['plot_dir'], 'anomalies_by_hour.png'))\n",
        "            plt.close()\n",
        "\n",
        "        # Analyze by day of week if data spans at least one week\n",
        "        if hasattr(anomalies.index, 'dayofweek') and (anomaly_results.index.max() - anomaly_results.index.min()).days >= 7:\n",
        "            day_counts = anomalies.groupby(anomalies.index.dayofweek).size()\n",
        "            days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "            day_dict = {i: days[i] for i in range(7)}\n",
        "            summary['day_distribution'] = {day_dict[k]: int(v) for k, v in day_counts.to_dict().items()}\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            day_counts.plot(kind='bar')\n",
        "            plt.title('Distribution of Anomalies by Day of Week')\n",
        "            plt.xlabel('Day of Week')\n",
        "            plt.ylabel('Count')\n",
        "            plt.xticks(ticks=range(7), labels=days, rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.config['plot_dir'], 'anomalies_by_day.png'))\n",
        "            plt.close()\n",
        "\n",
        "        # Identify clusters of consecutive anomalies as events\n",
        "        anomaly_results['event_group'] = (anomaly_results['anomaly'].diff() == 1).cumsum()\n",
        "        events = anomaly_results[anomaly_results['anomaly']].groupby('event_group')\n",
        "        events_list = []\n",
        "        for group, event_data in events:\n",
        "            if len(event_data) > 1:\n",
        "                event = {\n",
        "                    'start_time': event_data.index.min().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'end_time': event_data.index.max().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'duration_hours': (event_data.index.max() - event_data.index.min()).total_seconds() / 3600,\n",
        "                    'num_points': int(len(event_data)),\n",
        "                    'parameters': {}\n",
        "                }\n",
        "                for param in params:\n",
        "                    event['parameters'][param] = {\n",
        "                        'mean': float(event_data[param].mean()),\n",
        "                        'max': float(event_data[param].max()),\n",
        "                        'min': float(event_data[param].min())\n",
        "                    }\n",
        "                events_list.append(event)\n",
        "        summary['events'] = events_list\n",
        "\n",
        "        # Save summary report to JSON\n",
        "        summary_path = os.path.join(self.config['report_dir'], 'anomaly_analysis_summary.json')\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "        logger.info(f\"Anomaly analysis summary saved to {summary_path}\")\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Configuration parameters\n",
        "    config = {\n",
        "        'params': ['pm2.5cnc', 'pm10cnc'],   # Air quality parameters to analyze\n",
        "        'resample': True,                    # Resample data to regular intervals\n",
        "        'resample_freq': '1h',               # Frequency for resampling (hourly)\n",
        "        'lstm_sequence_length': 24,          # Number of timesteps per sequence\n",
        "        'lstm_epochs': 50,                   # Maximum number of epochs\n",
        "        'lstm_batch_size': 32,               # Batch size for training\n",
        "        'encoding_dim': 32,                  # LSTM encoding dimension\n",
        "        'dropout_rate': 0.2,                 # Dropout rate to reduce overfitting\n",
        "        'early_stopping_patience': 5,        # Patience for early stopping\n",
        "        'optimizer': 'adam',                 # Optimizer\n",
        "        'loss': 'mse',                       # Loss function\n",
        "        'anomaly_threshold_factor': 3,       # Factor for threshold = mean + factor * std (training error)\n",
        "        'train_ratio': 0.8,                  # 80/20 train-test split based on time\n",
        "        'model_dir': 'lstm_models',\n",
        "        'plot_dir': 'lstm_plots',\n",
        "        'report_dir': 'lstm_reports'\n",
        "    }\n",
        "\n",
        "    detector = LSTMAnomalyDetector(config)\n",
        "    detector.load_data('/content/drive/MyDrive/WCE/air_quality_data.csv')\n",
        "    detector.preprocess_data()\n",
        "    detector.train_lstm_autoencoder()\n",
        "    anomaly_results = detector.detect_anomalies()  # Detect anomalies on test data by default\n",
        "    detector.analyze_anomalies(anomaly_results)\n",
        "    logger.info(\"LSTM anomaly detection completed successfully\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want push this above code in my github\n",
        "\n",
        "# You can't directly push Colab notebook code to GitHub in the same way you push local files.\n",
        "# You need to save the notebook as a .ipynb file, then commit and push that file to your repository.\n",
        "\n",
        "\n",
        "# 1. Save the notebook:\n",
        "# Go to \"File\" -> \"Save\" in the Colab interface.  This saves the current state to your Google Drive.\n",
        "\n",
        "\n",
        "# 2. Download the .ipynb file\n",
        "# Go to \"File\" -> \"Download\" -> \"Download .ipynb\" to download the file locally.\n",
        "\n",
        "\n",
        "# 3. Use Git to add, commit, and push the file.  Assuming you have a git repository setup locally, you can now do these steps:\n",
        "#   - Open a terminal on your computer.\n",
        "#   - Navigate to the directory where you saved the .ipynb file.\n",
        "#   - git add <your_notebook_name.ipynb>\n",
        "#   - git commit -m \"Add Colab notebook\"\n",
        "#   - git push origin <your_branch_name>\n",
        "\n",
        "\n",
        "# Example:\n",
        "# !git add my_colab_notebook.ipynb\n",
        "# !git commit -m \"Add my Colab notebook to Github\"\n",
        "# !git push origin main  # Replace 'main' with your branch if needed.\n",
        "\n",
        "# Note that you need to have git configured on your local machine and linked to your GitHub repository.\n",
        "# You'll also need appropriate access permissions.\n"
      ],
      "metadata": {
        "id": "rBWGshTaRli2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}