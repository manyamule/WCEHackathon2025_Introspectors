{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWE6Vbx8DhZLBC6TRmhV/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manyamule/WCEHackathon2025_Introspectors/blob/main/TransNAS_TSAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xerKFadLx67g",
        "outputId": "45bb769c-f1bf-4918-9d7c-b05738677fab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpPyCApfxJS6",
        "outputId": "ac64faf1-df4e-4662-a459-aa8e96573537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, logging, json, joblib\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(\"TransformerAnomalyDetection\")\n",
        "\n",
        "# Define a simple Transformer-based autoencoder model\n",
        "class AnomalyTransformer(nn.Module):\n",
        "    def __init__(self, feature_size, seq_length, num_layers=2, nhead=4, dim_feedforward=128, dropout=0.1):\n",
        "        super(AnomalyTransformer, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.feature_size = feature_size\n",
        "\n",
        "        self.input_projection = nn.Linear(feature_size, dim_feedforward)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_feedforward, nhead=nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output_projection = nn.Linear(dim_feedforward, feature_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length, feature_size)\n",
        "        x = self.input_projection(x)  # -> (batch_size, seq_length, dim_feedforward)\n",
        "        # Transformer expects (seq_length, batch_size, d_model)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.output_projection(x)\n",
        "        return x\n",
        "\n",
        "# Transformer-based anomaly detector class\n",
        "class TransformerAnomalyDetector:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.data = None\n",
        "        self.preprocessed_data = None\n",
        "        self.train_data = None\n",
        "        self.test_data = None\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.threshold = None\n",
        "\n",
        "        os.makedirs(config['model_dir'], exist_ok=True)\n",
        "        os.makedirs(config['plot_dir'], exist_ok=True)\n",
        "        os.makedirs(config['report_dir'], exist_ok=True)\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        logger.info(f\"Loading data from {csv_path}\")\n",
        "        try:\n",
        "            self.data = pd.read_csv(csv_path)\n",
        "            if 'dt_time' in self.data.columns:\n",
        "                self.data['dt_time'] = pd.to_datetime(self.data['dt_time'])\n",
        "                self.data.set_index('dt_time', inplace=True)\n",
        "            logger.info(f\"Data loaded. Shape: {self.data.shape}\")\n",
        "            return self.data\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        logger.info(\"Preprocessing data\")\n",
        "        if self.data is None:\n",
        "            logger.error(\"No data loaded\")\n",
        "            return\n",
        "        df = self.data.copy()\n",
        "        # Select specified parameters if provided\n",
        "        params = self.config.get('params', [])\n",
        "        if params and all(p in df.columns for p in params):\n",
        "            logger.info(f\"Selecting parameters: {params}\")\n",
        "            df = df[params]\n",
        "        elif params:\n",
        "            available = [p for p in params if p in df.columns]\n",
        "            logger.warning(f\"Some parameters not found. Using available: {available}\")\n",
        "            df = df[available]\n",
        "        # Handle missing values\n",
        "        if df.isnull().sum().sum() > 0:\n",
        "            logger.info(\"Filling missing values\")\n",
        "            df.ffill(inplace=True)\n",
        "            df.bfill(inplace=True)\n",
        "        # Remove duplicate timestamps\n",
        "        if df.index.duplicated().any():\n",
        "            logger.info(\"Removing duplicate timestamps\")\n",
        "            df = df[~df.index.duplicated()]\n",
        "        # Resample if needed\n",
        "        if self.config.get('resample', False):\n",
        "            freq = self.config.get('resample_freq', '1h')\n",
        "            logger.info(f\"Resampling data to {freq}\")\n",
        "            df = df.resample(freq).mean()\n",
        "            df.ffill(inplace=True)\n",
        "            df.bfill(inplace=True)\n",
        "        self.preprocessed_data = df\n",
        "        logger.info(f\"Preprocessed data shape: {df.shape}\")\n",
        "        # Time-based train-test split\n",
        "        train_ratio = self.config.get('train_ratio', 0.8)\n",
        "        split_index = int(len(df) * train_ratio)\n",
        "        self.train_data = df.iloc[:split_index]\n",
        "        self.test_data = df.iloc[split_index:]\n",
        "        logger.info(f\"Train shape: {self.train_data.shape}, Test shape: {self.test_data.shape}\")\n",
        "        return df\n",
        "\n",
        "    def _create_sequences(self, data, seq_length):\n",
        "        X = []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            X.append(data[i:i+seq_length])\n",
        "        return np.array(X)\n",
        "\n",
        "    def _build_transformer_model(self, input_shape):\n",
        "        feature_size = input_shape[2]\n",
        "        seq_length = input_shape[1]\n",
        "        num_layers = self.config.get('num_layers', 2)\n",
        "        nhead = self.config.get('nhead', 4)\n",
        "        dim_feedforward = self.config.get('dim_feedforward', 128)\n",
        "        dropout = self.config.get('dropout', 0.1)\n",
        "        model = AnomalyTransformer(feature_size, seq_length, num_layers, nhead, dim_feedforward, dropout)\n",
        "        logger.info(\"Transformer model built:\")\n",
        "        logger.info(model)\n",
        "        return model.to(device)\n",
        "\n",
        "    def train_transformer_autoencoder(self):\n",
        "        logger.info(\"Training Transformer Autoencoder\")\n",
        "        if self.train_data is None:\n",
        "            logger.error(\"No training data available\")\n",
        "            return\n",
        "        df_train = self.train_data.copy()\n",
        "        self.scaler = StandardScaler()\n",
        "        scaled_train = self.scaler.fit_transform(df_train)\n",
        "        seq_length = self.config.get('sequence_length', 24)\n",
        "        X_train = self._create_sequences(scaled_train, seq_length)\n",
        "        logger.info(f\"Created {len(X_train)} training sequences\")\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "\n",
        "        self.model = self._build_transformer_model(X_train.shape)\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.config.get('learning_rate', 1e-3))\n",
        "        criterion = nn.MSELoss()\n",
        "        epochs = self.config.get('epochs', 50)\n",
        "        batch_size = self.config.get('batch_size', 32)\n",
        "\n",
        "        train_losses = []\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            permutation = torch.randperm(X_train_tensor.size(0))\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X_train_tensor.size(0), batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x = X_train_tensor[indices]\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = criterion(outputs, batch_x)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item() * batch_x.size(0)\n",
        "            epoch_loss /= X_train_tensor.size(0)\n",
        "            train_losses.append(epoch_loss)\n",
        "            logger.info(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}\")\n",
        "        self._plot_training_history(train_losses)\n",
        "\n",
        "        # Save model and scaler\n",
        "        model_path = os.path.join(self.config['model_dir'], 'transformer_autoencoder.pth')\n",
        "        scaler_path = os.path.join(self.config['model_dir'], 'transformer_scaler.joblib')\n",
        "        torch.save(self.model.state_dict(), model_path)\n",
        "        joblib.dump(self.scaler, scaler_path)\n",
        "        logger.info(f\"Model saved to {model_path}\")\n",
        "\n",
        "        # Compute reconstruction errors on training data to set dynamic threshold\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            reconstructions = self.model(X_train_tensor)\n",
        "            mse = torch.mean((X_train_tensor - reconstructions)**2, dim=(1,2)).cpu().numpy()\n",
        "        factor = self.config.get('anomaly_threshold_factor', 3)\n",
        "        self.threshold = np.mean(mse) + factor * np.std(mse)\n",
        "        logger.info(f\"Anomaly threshold set to {self.threshold}\")\n",
        "        return self.model\n",
        "\n",
        "    def detect_anomalies(self, data=None):\n",
        "        logger.info(\"Detecting anomalies using Transformer Autoencoder\")\n",
        "        if data is not None:\n",
        "            df = data.copy()\n",
        "        elif self.test_data is not None:\n",
        "            df = self.test_data.copy()\n",
        "        else:\n",
        "            logger.error(\"No data provided for detection\")\n",
        "            return None\n",
        "\n",
        "        scaled_data = self.scaler.transform(df)\n",
        "        seq_length = self.config.get('sequence_length', 24)\n",
        "        X = self._create_sequences(scaled_data, seq_length)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            reconstructions = self.model(X_tensor)\n",
        "            mse = torch.mean((X_tensor - reconstructions)**2, dim=(1,2)).cpu().numpy()\n",
        "        threshold = self.threshold if self.threshold is not None else np.percentile(mse, 100 - self.config.get('anomaly_percent', 1))\n",
        "        logger.info(f\"Using anomaly threshold: {threshold}\")\n",
        "\n",
        "        # Build results DataFrame (accounting for sequence length)\n",
        "        result = pd.DataFrame(index=df.index[seq_length:])\n",
        "        result['reconstruction_error'] = mse\n",
        "        result['anomaly'] = mse > threshold\n",
        "        for col in df.columns:\n",
        "            result[col] = df[col].values[seq_length:]\n",
        "        anomaly_count = result['anomaly'].sum()\n",
        "        logger.info(f\"Detected {anomaly_count} anomalies out of {len(result)} points\")\n",
        "        self._plot_anomaly_results(result)\n",
        "        result_path = os.path.join(self.config['report_dir'], 'transformer_anomalies.csv')\n",
        "        result.to_csv(result_path)\n",
        "        logger.info(f\"Anomaly detection results saved to {result_path}\")\n",
        "        return result\n",
        "\n",
        "    def _plot_training_history(self, losses):\n",
        "        plt.figure(figsize=(10,6))\n",
        "        plt.plot(losses, label='Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss History')\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(self.config['plot_dir'], 'transformer_training_history.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_anomaly_results(self, result_df):\n",
        "        plt.figure(figsize=(15,8))\n",
        "        # Plot reconstruction error with threshold\n",
        "        plt.subplot(2,1,1)\n",
        "        plt.plot(result_df.index, result_df['reconstruction_error'], label='Reconstruction Error')\n",
        "        plt.axhline(y=self.threshold, color='r', linestyle='--', label='Threshold')\n",
        "        plt.title('Reconstruction Error Over Time')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Error')\n",
        "        plt.legend()\n",
        "        # Plot one parameter with anomalies highlighted\n",
        "        plt.subplot(2,1,2)\n",
        "        params = self.config.get('params', [col for col in result_df.columns if col not in ['reconstruction_error', 'anomaly']])\n",
        "        if params:\n",
        "            param = params[0]\n",
        "            plt.plot(result_df.index, result_df[param], label=param)\n",
        "            anomalies = result_df[result_df['anomaly']]\n",
        "            plt.scatter(anomalies.index, anomalies[param], color='red', label='Anomaly', s=50)\n",
        "            plt.title(f\"{param} with Detected Anomalies\")\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(param)\n",
        "            plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.config['plot_dir'], 'transformer_anomalies_overview.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def analyze_anomalies(self, result_df):\n",
        "        if result_df is None or result_df.empty:\n",
        "            logger.error(\"No anomaly results to analyze\")\n",
        "            return\n",
        "        logger.info(\"Analyzing anomalies\")\n",
        "        anomalies = result_df[result_df['anomaly']]\n",
        "        summary = {\n",
        "            'total_points': len(result_df),\n",
        "            'anomaly_count': int(anomalies.shape[0]),\n",
        "            'anomaly_percent': float((anomalies.shape[0] / len(result_df)) * 100)\n",
        "        }\n",
        "        summary_path = os.path.join(self.config['report_dir'], 'transformer_anomaly_summary.json')\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "        logger.info(f\"Anomaly summary saved to {summary_path}\")\n",
        "        return summary\n",
        "\n",
        "def main():\n",
        "    # Configuration similar to your LSTM example\n",
        "    config = {\n",
        "        'params': ['pm2.5cnc', 'pm10cnc'],   # Adjust parameter names as per your CSV\n",
        "        'resample': True,\n",
        "        'resample_freq': '1h',\n",
        "        'sequence_length': 24,               # Sequence length (e.g., 24 timesteps)\n",
        "        'epochs': 50,\n",
        "        'batch_size': 32,\n",
        "        'num_layers': 2,\n",
        "        'nhead': 4,\n",
        "        'dim_feedforward': 128,\n",
        "        'dropout': 0.1,\n",
        "        'learning_rate': 1e-3,\n",
        "        'anomaly_threshold_factor': 3,       # e.g., threshold = mean + 3*std on training error\n",
        "        'train_ratio': 0.8,\n",
        "        'model_dir': 'transformer_models',\n",
        "        'plot_dir': 'transformer_plots',\n",
        "        'report_dir': 'transformer_reports'\n",
        "    }\n",
        "\n",
        "    detector = TransformerAnomalyDetector(config)\n",
        "    detector.load_data('/content/drive/MyDrive/WCE/air_quality_data.csv')\n",
        "    detector.preprocess_data()\n",
        "    detector.train_transformer_autoencoder()\n",
        "    anomaly_results = detector.detect_anomalies()\n",
        "    detector.analyze_anomalies(anomaly_results)\n",
        "    logger.info(\"Transformer anomaly detection completed successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the local directories (as used in your config)\n",
        "local_model_dir = 'transformer_models'\n",
        "local_plot_dir = 'transformer_plots'\n",
        "local_report_dir = 'transformer_reports'\n",
        "\n",
        "# Define the destination folder in your Google Drive\n",
        "drive_destination = '/content/drive/MyDrive/AnomalyDetectionResults'\n",
        "\n",
        "# Create destination directories if they don't exist\n",
        "os.makedirs(os.path.join(drive_destination, 'Models'), exist_ok=True)\n",
        "os.makedirs(os.path.join(drive_destination, 'Plots'), exist_ok=True)\n",
        "os.makedirs(os.path.join(drive_destination, 'Reports'), exist_ok=True)\n",
        "\n",
        "# Copy the directories to the Drive folder\n",
        "shutil.copytree(local_model_dir, os.path.join(drive_destination, 'Models'), dirs_exist_ok=True)\n",
        "shutil.copytree(local_plot_dir, os.path.join(drive_destination, 'Plots'), dirs_exist_ok=True)\n",
        "shutil.copytree(local_report_dir, os.path.join(drive_destination, 'Reports'), dirs_exist_ok=True)\n",
        "\n",
        "print(\"Files have been copied to your Google Drive folder:\", drive_destination)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS9R8ywcx71p",
        "outputId": "0b0c7b8a-a99d-4a06-d3cb-b4151c95162d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been copied to your Google Drive folder: /content/drive/MyDrive/AnomalyDetectionResults\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WIKYS_qm4pDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}