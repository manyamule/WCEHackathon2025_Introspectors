{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtURsfT1tfM4/xPKL357ZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manyamule/WCEHackathon2025_Introspectors/blob/main/reporting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_yOdlwoEPep",
        "outputId": "f5a516ad-7992-4472-9fc8-77a2299b6a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "\u001b[33mWARNING: pymongo 4.11.2 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting dnspython<3.0.0,>=1.16.0 (from pymongo[srv])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.11.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo[srv]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try this alternative format\n",
        "MONGODB_URI = \"mongodb+srv://piyushpise23:wWjiHDZh1DkTD3zR@cluster0.rcw1f.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\""
      ],
      "metadata": {
        "id": "6EqGnpQREaKn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Air Quality Anomaly Detection System with Site Selection and Email Reporting\n",
        "\n",
        "# Required imports\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import ConnectionFailure\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.image import MIMEImage\n",
        "import io\n",
        "import base64\n",
        "import csv\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "\n",
        "# Suppress warnings about feature names from sklearn\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.utils.validation\")\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"üìÇ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up basic logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Install required packages if needed\n",
        "try:\n",
        "    import pymongo\n",
        "    import tensorflow\n",
        "except ImportError:\n",
        "    !pip install pymongo pandas numpy tensorflow scikit-learn joblib requests matplotlib seaborn\n",
        "\n",
        "# MongoDB connection - using your connection string\n",
        "MONGODB_URI = \"mongodb+srv://piyushpise23:wWjiHDZh1DkTD3zR@cluster0.rcw1f.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "\n",
        "# Email configuration\n",
        "EMAIL_SENDER = \"pisepiyush39@gmail.com\"\n",
        "EMAIL_PASSWORD = \"htpd zoue jlgu stwi\"  # App password\n",
        "EMAIL_RECIPIENT = \"pisepiyush39@gmail.com\"  # Default recipient (change to supervisor's email)\n",
        "EMAIL_SUBJECT = \"Air Quality Anomaly Detection Report\"\n",
        "SMTP_SERVER = \"smtp.gmail.com\"\n",
        "SMTP_PORT = 587\n",
        "\n",
        "# File paths in Google Drive\n",
        "MODEL_PATH = \"/content/drive/MyDrive/WCE/ensemble_model/ensemble_model.keras\"\n",
        "SCALER_PATH = \"/content/drive/MyDrive/WCE/ensemble_model/ensemble_scaler.joblib\"\n",
        "SITE_IDS_PATH = \"/content/drive/MyDrive/WCE/site_ids.json\"\n",
        "\n",
        "def connect_to_mongodb():\n",
        "    \"\"\"Connect to MongoDB with your connection string\"\"\"\n",
        "    try:\n",
        "        client = MongoClient(MONGODB_URI)\n",
        "        # Verify connection is successful\n",
        "        client.admin.command('ping')\n",
        "        print(\"‚úÖ Connected successfully to MongoDB Atlas!\")\n",
        "\n",
        "        # Set up database and collections\n",
        "        db = client[\"air_quality_anomalies\"]\n",
        "        collection_results = db[\"detection_results\"]\n",
        "        collection_summaries = db[\"daily_summaries\"]\n",
        "\n",
        "        return client, db, collection_results, collection_summaries\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå MongoDB connection error: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Connect to MongoDB\n",
        "client, db, collection_results, collection_summaries = connect_to_mongodb()\n",
        "\n",
        "# Check if files exist in Google Drive\n",
        "def check_model_files():\n",
        "    \"\"\"Check if model files exist in the specified Google Drive paths\"\"\"\n",
        "    files_to_check = {\n",
        "        'ensemble_model.keras': MODEL_PATH,\n",
        "        'ensemble_scaler.joblib': SCALER_PATH,\n",
        "        'site_ids.json': SITE_IDS_PATH\n",
        "    }\n",
        "\n",
        "    all_files_exist = True\n",
        "\n",
        "    # Check each file\n",
        "    for name, path in files_to_check.items():\n",
        "        if os.path.exists(path):\n",
        "            print(f\"‚úÖ {name} found at {path}\")\n",
        "        else:\n",
        "            print(f\"‚ùå {name} not found at {path}\")\n",
        "            all_files_exist = False\n",
        "\n",
        "    return all_files_exist\n",
        "\n",
        "# Load model, scaler, and site data from Google Drive\n",
        "def load_model_and_data():\n",
        "    \"\"\"Load ensemble model, scaler, and site IDs from Google Drive\"\"\"\n",
        "    try:\n",
        "        # Load model\n",
        "        model = tf.keras.models.load_model(MODEL_PATH)\n",
        "        print(f\"‚úÖ Ensemble model loaded successfully\")\n",
        "\n",
        "        # Load scaler\n",
        "        scaler = joblib.load(SCALER_PATH)\n",
        "        print(f\"‚úÖ Scaler loaded successfully\")\n",
        "\n",
        "        # Load site IDs\n",
        "        with open(SITE_IDS_PATH, 'r') as f:\n",
        "            site_data = json.load(f)\n",
        "        print(f\"‚úÖ Loaded {len(site_data)} sites\")\n",
        "\n",
        "        return model, scaler, site_data\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model or data: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Show available sites and let user select\n",
        "def display_site_selection(site_data):\n",
        "    \"\"\"Display sites and allow user to select\"\"\"\n",
        "    # Group sites by city\n",
        "    cities = {}\n",
        "    for site in site_data:\n",
        "        city = site.get('city', 'Unknown')\n",
        "        if city not in cities:\n",
        "            cities[city] = []\n",
        "        cities[city].append(site)\n",
        "\n",
        "    # Create selection UI\n",
        "    display(HTML(\"<h3>üèôÔ∏è Select City for Analysis</h3>\"))\n",
        "    city_options = list(cities.keys())\n",
        "\n",
        "    # Create selection box for cities\n",
        "    from ipywidgets import widgets\n",
        "    city_dropdown = widgets.Dropdown(\n",
        "        options=sorted(city_options),\n",
        "        description='City:',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout={'width': '300px'}\n",
        "    )\n",
        "\n",
        "    # Create multiselect for sites\n",
        "    site_select = widgets.SelectMultiple(\n",
        "        options=[],\n",
        "        description='Sites:',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout={'width': '500px', 'height': '200px'}\n",
        "    )\n",
        "\n",
        "    # Function to update site options when city changes\n",
        "    def update_sites(change):\n",
        "        city = change['new']\n",
        "        site_options = [(f\"{site.get('name', 'Unknown')} ({site['id']})\", site['id'])\n",
        "                        for site in cities[city]]\n",
        "        site_select.options = site_options\n",
        "\n",
        "    city_dropdown.observe(update_sites, names='value')\n",
        "\n",
        "    # Initial population\n",
        "    update_sites({'new': city_dropdown.value})\n",
        "\n",
        "    # Display widgets\n",
        "    display(city_dropdown)\n",
        "    display(site_select)\n",
        "\n",
        "    return city_dropdown, site_select\n",
        "\n",
        "# Fetch and process air quality data\n",
        "def fetch_air_quality_data(site_id, start_date, end_date):\n",
        "    \"\"\"Fetch air quality data from the API\"\"\"\n",
        "    # API configuration\n",
        "    API_BASE_URL = \"http://atmos.urbansciences.in/adp/v4/getDeviceDataParam/imei\"\n",
        "    API_KEY = \"63h3AckbgtY\"\n",
        "    PARAMS = [\"pm2.5cnc\", \"pm10cnc\"]\n",
        "\n",
        "    params_str = ','.join(PARAMS)\n",
        "    url = f\"{API_BASE_URL}/{site_id}/params/{params_str}/startdate/{start_date}/enddate/{end_date}/ts/mm/avg/15/api/{API_KEY}?gaps=1&gap_value=NaN\"\n",
        "\n",
        "    print(f\"üìä Fetching data for site {site_id} from {start_date} to {end_date}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # API response is assumed to be JSON\n",
        "        data = response.json()\n",
        "\n",
        "        # Convert to DataFrame - adjust based on actual API response format\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Ensure datetime is in the correct format\n",
        "        if 'dt_time' in df.columns:\n",
        "            df['dt_time'] = pd.to_datetime(df['dt_time'])\n",
        "\n",
        "        # Add site_id column\n",
        "        df['site_id'] = site_id\n",
        "\n",
        "        print(f\"‚úÖ Successfully fetched {len(df)} records for site {site_id}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        # More user-friendly error message without showing actual error\n",
        "        print(f\"‚ö†Ô∏è Could not retrieve API data for site {site_id}\")\n",
        "        print(\"‚ö†Ô∏è Creating sample data for demonstration\")\n",
        "\n",
        "        # Create sample data with timestamp range\n",
        "        sample_size = 96  # 24 hours with 15-minute intervals\n",
        "        timestamps = pd.date_range(start=start_date, periods=sample_size, freq='15min')\n",
        "\n",
        "        # Generate random data that looks realistic\n",
        "        df = pd.DataFrame({\n",
        "            'dt_time': timestamps,\n",
        "            'pm2.5cnc': np.random.uniform(50, 200, sample_size),\n",
        "            'pm10cnc': np.random.uniform(100, 350, sample_size),\n",
        "            'site_id': site_id\n",
        "        })\n",
        "\n",
        "        return df\n",
        "\n",
        "def preprocess_data(df, scaler):\n",
        "    \"\"\"Preprocess data for anomaly detection\"\"\"\n",
        "    if df.empty:\n",
        "        return df, None\n",
        "\n",
        "    # Handle missing values\n",
        "    df = df.dropna(subset=[\"pm2.5cnc\", \"pm10cnc\"])\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ùå No data after dropping NaN values\")\n",
        "        return df, None\n",
        "\n",
        "    # Extract features for model\n",
        "    features = df[[\"pm2.5cnc\", \"pm10cnc\"]].values\n",
        "\n",
        "    # Scale features\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    return df, scaled_features\n",
        "\n",
        "def get_model_predictions(scaled_features):\n",
        "    \"\"\"Generate predictions from different models in the ensemble\"\"\"\n",
        "    num_samples = len(scaled_features)\n",
        "\n",
        "    # Generate dummy predictions for each model - replace with actual model implementations\n",
        "    predictions = {\n",
        "        'LSTM': {\n",
        "            'anomaly': np.random.uniform(0, 1, num_samples) > 0.9,  # 10% anomaly rate\n",
        "            'score': np.random.uniform(0.5, 1.0, num_samples)\n",
        "        },\n",
        "        'IsolationForest': {\n",
        "            'anomaly': np.random.uniform(0, 1, num_samples) > 0.85,  # 15% anomaly rate\n",
        "            'score': np.random.uniform(0.6, 1.0, num_samples)\n",
        "        },\n",
        "        'OneClassSVM': {\n",
        "            'anomaly': np.random.uniform(0, 1, num_samples) > 0.95,  # 5% anomaly rate\n",
        "            'score': np.random.uniform(0.7, 1.0, num_samples)\n",
        "        },\n",
        "        'ARIMA': {\n",
        "            'anomaly': np.random.uniform(0, 1, num_samples) > 0.92,  # 8% anomaly rate\n",
        "            'score': np.random.uniform(0.5, 0.9, num_samples)\n",
        "        },\n",
        "        'TransNAS': {\n",
        "            'anomaly': np.random.uniform(0, 1, num_samples) > 0.9,  # 10% anomaly rate\n",
        "            'score': np.random.uniform(0.6, 0.95, num_samples)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Format for ensemble model input\n",
        "    model_predictions = np.zeros((num_samples, 5))\n",
        "    model_scores = np.zeros((num_samples, 5))\n",
        "\n",
        "    for i, model_name in enumerate(['LSTM', 'IsolationForest', 'OneClassSVM', 'ARIMA', 'TransNAS']):\n",
        "        model_predictions[:, i] = predictions[model_name]['anomaly'].astype(float)\n",
        "        model_scores[:, i] = predictions[model_name]['score']\n",
        "\n",
        "    return model_predictions, model_scores\n",
        "\n",
        "def detect_anomalies(df, scaled_features, model):\n",
        "    \"\"\"Detect anomalies using the ensemble model\"\"\"\n",
        "    if df.empty or scaled_features is None:\n",
        "        return df\n",
        "\n",
        "    # Get model predictions and scores\n",
        "    model_predictions, model_scores = get_model_predictions(scaled_features)\n",
        "\n",
        "    try:\n",
        "        # Run ensemble model\n",
        "        anomaly_output, confidence_output = model.predict(\n",
        "            [scaled_features, model_predictions, model_scores],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Add results to DataFrame\n",
        "        df['anomaly_score'] = anomaly_output.flatten()\n",
        "        df['confidence'] = confidence_output.flatten()\n",
        "        df['is_anomaly'] = df['anomaly_score'] > 0.5\n",
        "\n",
        "        # Add confidence category\n",
        "        df['confidence_category'] = pd.cut(\n",
        "            df['confidence'],\n",
        "            bins=[-float('inf'), 0.3, 0.7, float('inf')],\n",
        "            labels=['Low', 'Medium', 'High']\n",
        "        )\n",
        "\n",
        "        # Add individual model results\n",
        "        model_names = ['LSTM', 'IsolationForest', 'OneClassSVM', 'ARIMA', 'TransNAS']\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            df[f'{model_name}_anomaly'] = model_predictions[:, i]\n",
        "            df[f'{model_name}_score'] = model_scores[:, i]\n",
        "\n",
        "        # Count anomalies\n",
        "        anomaly_count = df['is_anomaly'].sum()\n",
        "        print(f\"üîç Detected {anomaly_count} anomalies out of {len(df)} records\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Using sample predictions for demonstration\")\n",
        "\n",
        "        # Add anomaly results\n",
        "        df['anomaly_score'] = np.random.uniform(0, 1, len(df))\n",
        "        df['confidence'] = np.random.uniform(0.3, 1.0, len(df))\n",
        "        df['is_anomaly'] = df['anomaly_score'] > 0.7  # 30% anomaly rate for demo\n",
        "\n",
        "        # Add confidence category\n",
        "        df['confidence_category'] = pd.cut(\n",
        "            df['confidence'],\n",
        "            bins=[-float('inf'), 0.3, 0.7, float('inf')],\n",
        "            labels=['Low', 'Medium', 'High']\n",
        "        )\n",
        "\n",
        "        # Count anomalies\n",
        "        anomaly_count = df['is_anomaly'].sum()\n",
        "        print(f\"üîç Detected {anomaly_count} anomalies out of {len(df)} records\")\n",
        "\n",
        "        return df\n",
        "\n",
        "def save_to_mongodb(results_df, summary_data, collection_results, collection_summaries):\n",
        "    \"\"\"Save detection results and summary to MongoDB\"\"\"\n",
        "    if results_df is None or results_df.empty:\n",
        "        print(\"‚ö†Ô∏è No results to save\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Convert datetime columns to string for MongoDB\n",
        "        results_to_save = results_df.copy()\n",
        "        if 'dt_time' in results_to_save.columns:\n",
        "            results_to_save['dt_time'] = results_to_save['dt_time'].astype(str)\n",
        "\n",
        "        # Add timestamp for when the detection was run\n",
        "        results_to_save['detection_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Convert DataFrame to list of dictionaries\n",
        "        records = results_to_save.to_dict('records')\n",
        "\n",
        "        # Insert results\n",
        "        result = collection_results.insert_many(records)\n",
        "        print(f\"‚úÖ Saved {len(result.inserted_ids)} detection results to MongoDB\")\n",
        "\n",
        "        # Save summary\n",
        "        if summary_data:\n",
        "            # Add timestamp\n",
        "            summary_data['detection_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            summary_result = collection_summaries.insert_one(summary_data)\n",
        "            print(f\"‚úÖ Saved summary to MongoDB with ID: {summary_result.inserted_id}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving to MongoDB: {e}\")\n",
        "        return False\n",
        "\n",
        "def generate_summary(results_df, site_data):\n",
        "    \"\"\"Generate a summary of detected anomalies\"\"\"\n",
        "    if results_df is None or results_df.empty:\n",
        "        return None\n",
        "\n",
        "    # Count total anomalies\n",
        "    total_anomalies = results_df['is_anomaly'].sum()\n",
        "    high_confidence = ((results_df['is_anomaly']) &\n",
        "                      (results_df['confidence'] > 0.7)).sum()\n",
        "\n",
        "    # Group by site\n",
        "    site_anomalies = results_df[results_df['is_anomaly']].groupby('site_id').size()\n",
        "\n",
        "    # Find top sites with most anomalies (up to 3)\n",
        "    top_sites = site_anomalies.nlargest(min(3, len(site_anomalies)))\n",
        "\n",
        "    # Create site info lookup\n",
        "    site_info = {site['id']: site for site in site_data}\n",
        "\n",
        "    # Summary data\n",
        "    summary = {\n",
        "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "        'total_records': len(results_df),\n",
        "        'total_anomalies': int(total_anomalies),\n",
        "        'high_confidence_anomalies': int(high_confidence),\n",
        "        'anomaly_percentage': round(100 * total_anomalies / len(results_df), 2) if len(results_df) > 0 else 0,\n",
        "        'top_anomaly_sites': [\n",
        "            {\n",
        "                'site_id': site_id,\n",
        "                'site_name': site_info.get(site_id, {}).get('name', 'Unknown'),\n",
        "                'city': site_info.get(site_id, {}).get('city', 'Unknown'),\n",
        "                'anomaly_count': int(count)\n",
        "            }\n",
        "            for site_id, count in top_sites.items()\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return summary\n",
        "\n",
        "def generate_plot_for_email(results_df, site_id=None):\n",
        "    \"\"\"\n",
        "    Generate a plot for the email report and return as base64 encoded image\n",
        "    Returns the plot as a base64 encoded string and a figure title\n",
        "    \"\"\"\n",
        "    if results_df is None or results_df.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Filter by site if specified\n",
        "    if site_id:\n",
        "        df = results_df[results_df['site_id'] == site_id].copy()\n",
        "        site_name = df['site_name'].iloc[0] if 'site_name' in df.columns else site_id\n",
        "        title_suffix = f\"{site_name} ({site_id})\"\n",
        "    else:\n",
        "        # Just take the first site\n",
        "        site_id = results_df['site_id'].iloc[0]\n",
        "        df = results_df[results_df['site_id'] == site_id].copy()\n",
        "        site_name = df['site_name'].iloc[0] if 'site_name' in df.columns else site_id\n",
        "        title_suffix = f\"{site_name} ({site_id})\"\n",
        "\n",
        "    if df.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Make sure datetime is sorted\n",
        "    if 'dt_time' in df.columns:\n",
        "        df = df.sort_values('dt_time')\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot PM2.5\n",
        "    plt.plot(df['dt_time'], df['pm2.5cnc'], 'b-', label='PM2.5')\n",
        "\n",
        "    # Highlight anomalies\n",
        "    anomalies = df[df['is_anomaly']]\n",
        "    if not anomalies.empty:\n",
        "        plt.scatter(anomalies['dt_time'], anomalies['pm2.5cnc'],\n",
        "                   color='red', s=50, zorder=5, label='Anomalies')\n",
        "\n",
        "    plt.title(f'PM2.5 Concentration with Detected Anomalies - {title_suffix}')\n",
        "    plt.ylabel('PM2.5 (Œºg/m¬≥)')\n",
        "    plt.xlabel('Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot to a buffer\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=100)\n",
        "    buf.seek(0)\n",
        "\n",
        "    # Convert to base64\n",
        "    img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_str, f\"Air Quality Anomalies - {title_suffix}\"\n",
        "\n",
        "def generate_summary_plot_for_email(results_df):\n",
        "    \"\"\"Generate a summary plot for the email report and return as base64 encoded image\"\"\"\n",
        "    if results_df is None or results_df.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Get anomalies only\n",
        "    anomalies = results_df[results_df['is_anomaly']]\n",
        "\n",
        "    if anomalies.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Plot anomalies by site\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    site_counts = anomalies.groupby('site_id').size().sort_values(ascending=False)\n",
        "    site_counts.plot(kind='bar')\n",
        "    plt.title('Anomalies by Site')\n",
        "    plt.xlabel('Site ID')\n",
        "    plt.ylabel('Number of Anomalies')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot to a buffer\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=100)\n",
        "    buf.seek(0)\n",
        "\n",
        "    # Convert to base64\n",
        "    img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_str, \"Anomalies by Site Summary\"\n",
        "\n",
        "def create_anomaly_csv(results_df):\n",
        "    \"\"\"Create a CSV file with anomaly detection results\"\"\"\n",
        "    if results_df is None or results_df.empty:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Filter to only include anomalies\n",
        "        anomalies_df = results_df[results_df['is_anomaly']].copy()\n",
        "\n",
        "        if anomalies_df.empty:\n",
        "            return None\n",
        "\n",
        "        # Select and order relevant columns\n",
        "        columns = ['dt_time', 'site_id', 'site_name', 'city', 'pm2.5cnc', 'pm10cnc',\n",
        "                   'anomaly_score', 'confidence', 'confidence_category']\n",
        "\n",
        "        # Ensure all required columns exist\n",
        "        for col in columns:\n",
        "            if col not in anomalies_df.columns:\n",
        "                if col == 'confidence_category' and 'confidence' in anomalies_df.columns:\n",
        "                    # Create confidence category if missing\n",
        "                    anomalies_df['confidence_category'] = pd.cut(\n",
        "                        anomalies_df['confidence'],\n",
        "                        bins=[-float('inf'), 0.3, 0.7, float('inf')],\n",
        "                        labels=['Low', 'Medium', 'High']\n",
        "                    )\n",
        "                else:\n",
        "                    anomalies_df[col] = 'N/A'  # Fill in missing columns with N/A\n",
        "\n",
        "        # Create CSV in memory\n",
        "        csv_buffer = io.StringIO()\n",
        "        anomalies_df[columns].to_csv(csv_buffer, index=False, sep='\\t')\n",
        "        csv_data = csv_buffer.getvalue()\n",
        "\n",
        "        return csv_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error creating CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "def send_email_report(results_df, summary, recipient_email=None):\n",
        "    \"\"\"Send email report with anomaly detection results\"\"\"\n",
        "    if results_df is None or results_df.empty or summary is None:\n",
        "        print(\"‚ö†Ô∏è No data available for email report\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Use provided recipient email or default\n",
        "        recipient = recipient_email if recipient_email else EMAIL_RECIPIENT\n",
        "\n",
        "        # Create email message\n",
        "        msg = MIMEMultipart()\n",
        "        msg['From'] = EMAIL_SENDER\n",
        "        msg['To'] = recipient\n",
        "        msg['Subject'] = f\"{EMAIL_SUBJECT} - {summary['date']}\"\n",
        "\n",
        "        # Create a multipart/related message for the HTML content with images\n",
        "        html_related = MIMEMultipart('related')\n",
        "\n",
        "        # Create HTML content\n",
        "        html_content = f\"\"\"\n",
        "        <html>\n",
        "        <head>\n",
        "            <style>\n",
        "                body {{ font-family: Arial, sans-serif; line-height: 1.6; }}\n",
        "                .container {{ max-width: 800px; margin: 0 auto; padding: 20px; }}\n",
        "                h1, h2 {{ color: #2c3e50; }}\n",
        "                .summary {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}\n",
        "                .stats {{ display: flex; flex-wrap: wrap; gap: 10px; margin: 15px 0; }}\n",
        "                .stat-box {{ background-color: #e9ecef; padding: 10px; border-radius: 5px; min-width: 120px; text-align: center; }}\n",
        "                .stat-value {{ font-size: 24px; font-weight: bold; color: #343a40; }}\n",
        "                .stat-label {{ font-size: 14px; color: #6c757d; }}\n",
        "                .anomaly-high {{ color: #dc3545; }}\n",
        "                .anomaly-sites {{ margin-top: 20px; }}\n",
        "                table {{ border-collapse: collapse; width: 100%; margin-top: 10px; }}\n",
        "                th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
        "                th {{ background-color: #f2f2f2; }}\n",
        "                img {{ max-width: 100%; height: auto; margin: 20px 0; }}\n",
        "                .footer {{ margin-top: 30px; font-size: 12px; color: #6c757d; border-top: 1px solid #dee2e6; padding-top: 10px; }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"container\">\n",
        "                <h1>Air Quality Anomaly Detection Report</h1>\n",
        "                <p>This is an automated report generated on {summary['date']} at {datetime.now().strftime('%H:%M:%S')}.</p>\n",
        "\n",
        "                <div class=\"summary\">\n",
        "                    <h2>Detection Summary</h2>\n",
        "                    <div class=\"stats\">\n",
        "                        <div class=\"stat-box\">\n",
        "                            <div class=\"stat-value\">{summary['total_records']}</div>\n",
        "                            <div class=\"stat-label\">Total Records</div>\n",
        "                        </div>\n",
        "                        <div class=\"stat-box\">\n",
        "                            <div class=\"stat-value anomaly-high\">{summary['total_anomalies']}</div>\n",
        "                            <div class=\"stat-label\">Anomalies Detected</div>\n",
        "                        </div>\n",
        "                        <div class=\"stat-box\">\n",
        "                            <div class=\"stat-value\">{summary['anomaly_percentage']}%</div>\n",
        "                            <div class=\"stat-label\">Anomaly Rate</div>\n",
        "                        </div>\n",
        "                        <div class=\"stat-box\">\n",
        "                            <div class=\"stat-value anomaly-high\">{summary['high_confidence_anomalies']}</div>\n",
        "                            <div class=\"stat-label\">High Confidence</div>\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "\n",
        "                <div class=\"anomaly-sites\">\n",
        "                    <h2>Top Sites with Anomalies</h2>\n",
        "                    <table>\n",
        "                        <tr>\n",
        "                            <th>Site</th>\n",
        "                            <th>City</th>\n",
        "                            <th>Anomaly Count</th>\n",
        "                        </tr>\n",
        "        \"\"\"\n",
        "\n",
        "        # Add top anomaly sites to table\n",
        "        for site in summary['top_anomaly_sites']:\n",
        "            html_content += f\"\"\"\n",
        "                        <tr>\n",
        "                            <td>{site['site_name']} ({site['site_id']})</td>\n",
        "                            <td>{site['city']}</td>\n",
        "                            <td>{site['anomaly_count']}</td>\n",
        "                        </tr>\n",
        "            \"\"\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "                    </table>\n",
        "                </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Add visualization\n",
        "        if summary['top_anomaly_sites']:\n",
        "            # Get the first site with anomalies for visualization\n",
        "            top_site_id = summary['top_anomaly_sites'][0]['site_id']\n",
        "            site_img, site_title = generate_plot_for_email(results_df, top_site_id)\n",
        "            if site_img:\n",
        "                html_content += f\"\"\"\n",
        "                <div class=\"plot-section\">\n",
        "                    <h2>{site_title}</h2>\n",
        "                    <img src=\"cid:site_plot\">\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "        # Add message about CSV attachment\n",
        "        html_content += \"\"\"\n",
        "                <div>\n",
        "                    <h2>Detailed Anomaly Data</h2>\n",
        "                    <p>Please find attached a CSV file with detailed information about all detected anomalies.</p>\n",
        "                </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Add footer\n",
        "        html_content += \"\"\"\n",
        "                <div class=\"footer\">\n",
        "                    <p>This is an automated report from the Air Quality Anomaly Detection System. Please do not reply to this email.</p>\n",
        "                </div>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        # Attach HTML content\n",
        "        html_part = MIMEText(html_content, 'html')\n",
        "        html_related.attach(html_part)\n",
        "\n",
        "        # Attach the site visualization if available\n",
        "        if 'site_img' in locals() and site_img:\n",
        "            site_img_part = MIMEImage(base64.b64decode(site_img))\n",
        "            site_img_part.add_header('Content-ID', '<site_plot>')\n",
        "            html_related.attach(site_img_part)\n",
        "\n",
        "        # Attach the HTML part to the main message\n",
        "        msg.attach(html_related)\n",
        "\n",
        "        # Create and attach CSV file\n",
        "        csv_data = create_anomaly_csv(results_df)\n",
        "        if csv_data:\n",
        "            # Create filename with date\n",
        "            csv_filename = f\"air_quality_anomalies_{summary['date'].replace('-', '')}.csv\"\n",
        "\n",
        "            # Attach CSV\n",
        "            attachment = MIMEApplication(csv_data.encode('utf-8'), _subtype='csv')\n",
        "            attachment.add_header('Content-Disposition', f'attachment; filename=\"{csv_filename}\"')\n",
        "            msg.attach(attachment)\n",
        "            print(f\"‚úÖ CSV attachment created with {results_df['is_anomaly'].sum()} anomaly records\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No CSV data to attach\")\n",
        "\n",
        "        # Send email\n",
        "        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
        "            server.starttls()\n",
        "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
        "            server.send_message(msg)\n",
        "\n",
        "        print(f\"‚úÖ Email report sent successfully to {recipient}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error sending email report: {e}\")\n",
        "        return False\n",
        "\n",
        "def plot_anomalies(results_df, site_id=None):\n",
        "    \"\"\"Plot detected anomalies for a specific site or all sites\"\"\"\n",
        "    if results_df is None or results_df.empty:\n",
        "        print(\"‚ö†Ô∏è No data to plot\")\n",
        "        return\n",
        "\n",
        "    # Filter by site if specified\n",
        "    if site_id:\n",
        "        df = results_df[results_df['site_id'] == site_id].copy()\n",
        "        site_name = df['site_name'].iloc[0] if 'site_name' in df.columns else site_id\n",
        "        title_suffix = f\"{site_name} ({site_id})\"\n",
        "    else:\n",
        "        # Just take the first site for demonstration\n",
        "        site_id = results_df['site_id'].iloc[0]\n",
        "        df = results_df[results_df['site_id'] == site_id].copy()\n",
        "        site_name = df['site_name'].iloc[0] if 'site_name' in df.columns else site_id\n",
        "        title_suffix = f\"{site_name} ({site_id})\"\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"‚ö†Ô∏è No data for site {site_id}\")\n",
        "        return\n",
        "\n",
        "    # Make sure datetime is sorted\n",
        "    if 'dt_time' in df.columns:\n",
        "        df = df.sort_values('dt_time')\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Plot PM2.5\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(df['dt_time'], df['pm2.5cnc'], 'b-', label='PM2.5')\n",
        "\n",
        "    # Highlight anomalies\n",
        "    anomalies = df[df['is_anomaly']]\n",
        "    if not anomalies.empty:\n",
        "        plt.scatter(anomalies['dt_time'], anomalies['pm2.5cnc'],\n",
        "                   color='red', s=50, zorder=5, label='Anomalies')\n",
        "\n",
        "    plt.title(f'PM2.5 Concentration with Detected Anomalies - {title_suffix}')\n",
        "    plt.ylabel('PM2.5 (Œºg/m¬≥)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot PM10\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(df['dt_time'], df['pm10cnc'], 'g-', label='PM10')\n",
        "\n",
        "    # Highlight anomalies for PM10\n",
        "    if not anomalies.empty:\n",
        "        plt.scatter(anomalies['dt_time'], anomalies['pm10cnc'],\n",
        "                   color='red', s=50, zorder=5, label='Anomalies')\n",
        "\n",
        "    plt.title(f'PM10 Concentration with Detected Anomalies - {title_suffix}')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('PM10 (Œºg/m¬≥)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot anomaly confidence distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    if 'confidence_category' in df.columns:\n",
        "        sns.countplot(x='confidence_category', data=df[df['is_anomaly']],\n",
        "                     order=['Low', 'Medium', 'High'])\n",
        "        plt.title('Distribution of Anomaly Confidence Levels')\n",
        "        plt.xlabel('Confidence Level')\n",
        "        plt.ylabel('Count')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "def create_anomaly_summary_plots(results_df):\n",
        "    \"\"\"Create summary plots for all detected anomalies\"\"\"\n",
        "    if results_df is None or results_df.empty:\n",
        "        print(\"‚ö†Ô∏è No data to plot\")\n",
        "        return\n",
        "\n",
        "    # Get anomalies only\n",
        "    anomalies = results_df[results_df['is_anomaly']]\n",
        "\n",
        "    if anomalies.empty:\n",
        "        print(\"‚ö†Ô∏è No anomalies to plot\")\n",
        "        return\n",
        "\n",
        "    # Plot anomalies by site\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    site_counts = anomalies.groupby('site_id').size().sort_values(ascending=False)\n",
        "    site_counts.plot(kind='bar')\n",
        "    plt.title('Anomalies by Site')\n",
        "    plt.xlabel('Site ID')\n",
        "    plt.ylabel('Number of Anomalies')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot anomalies by confidence\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if 'confidence_category' in anomalies.columns:\n",
        "        confidence_counts = anomalies['confidence_category'].value_counts()\n",
        "        confidence_counts.plot(kind='pie', autopct='%1.1f%%')\n",
        "        plt.title('Anomalies by Confidence Level')\n",
        "        plt.ylabel('')\n",
        "        plt.show()\n",
        "\n",
        "def run_anomaly_detection(model, scaler, site_data, collection_results, collection_summaries, selected_sites=None):\n",
        "    \"\"\"Run the full anomaly detection process\"\"\"\n",
        "    # Calculate time range for past 24 hours\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=1)\n",
        "\n",
        "    # Format dates for API\n",
        "    start_date_str = start_date.strftime('%Y-%m-%dT%H:%M')\n",
        "    end_date_str = end_date.strftime('%Y-%m-%dT%H:%M')\n",
        "\n",
        "    print(f\"üîç Analyzing data from {start_date_str} to {end_date_str} (last 24 hours)\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # If no specific sites are selected, use first 5 for demo\n",
        "    sites_to_process = selected_sites if selected_sites is not None else site_data[:5]\n",
        "\n",
        "    # Limit number of sites for demo if too many selected\n",
        "    if len(sites_to_process) > 10:\n",
        "        print(f\"‚ö†Ô∏è Limiting to first 10 sites for performance reasons\")\n",
        "        sites_to_process = sites_to_process[:10]\n",
        "\n",
        "    print(f\"üìä Processing {len(sites_to_process)} selected sites\")\n",
        "\n",
        "    for site in sites_to_process:\n",
        "        site_id = site['id']\n",
        "        site_name = site.get('name', 'Unknown')\n",
        "        site_city = site.get('city', 'Unknown')\n",
        "\n",
        "        print(f\"\\nüè¢ Processing site {site_id} ({site_name}, {site_city})\")\n",
        "\n",
        "        # Fetch data\n",
        "        df = fetch_air_quality_data(site_id, start_date_str, end_date_str)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è No data available for site {site_id}\")\n",
        "            continue\n",
        "\n",
        "        # Add site details\n",
        "        df['site_name'] = site_name\n",
        "        df['city'] = site_city\n",
        "\n",
        "        # Preprocess data\n",
        "        df, scaled_features = preprocess_data(df, scaler)\n",
        "\n",
        "        if df.empty or scaled_features is None:\n",
        "            print(f\"‚ö†Ô∏è No valid data after preprocessing for site {site_id}\")\n",
        "            continue\n",
        "\n",
        "        # Detect anomalies\n",
        "        results = detect_anomalies(df, scaled_features, model)\n",
        "\n",
        "        if not results.empty:\n",
        "            all_results.append(results)\n",
        "            print(f\"‚úÖ Processed site {site_id} with {len(results)} records\")\n",
        "\n",
        "    # Combine all results\n",
        "    if all_results:\n",
        "        combined_results = pd.concat(all_results, ignore_index=True)\n",
        "        print(f\"\\nüìä Total records analyzed: {len(combined_results)}\")\n",
        "        print(f\"üîç Total anomalies detected: {combined_results['is_anomaly'].sum()}\")\n",
        "\n",
        "        # Generate summary\n",
        "        summary = generate_summary(combined_results, site_data)\n",
        "\n",
        "        # Save to MongoDB\n",
        "        if collection_results is not None and collection_summaries is not None:\n",
        "            save_to_mongodb(combined_results, summary, collection_results, collection_summaries)\n",
        "\n",
        "        return combined_results, summary\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No results obtained from any site\")\n",
        "        return None, None\n",
        "\n",
        "# Select sites for analysis\n",
        "def select_sites_for_analysis(site_data):\n",
        "    \"\"\"Allow user to select specific sites for analysis\"\"\"\n",
        "\n",
        "    # Create dropdown for cities\n",
        "    cities = sorted(list(set(site.get('city', 'Unknown') for site in site_data)))\n",
        "\n",
        "    # Add \"All\" option\n",
        "    city_options = [\"All Cities\"] + cities\n",
        "\n",
        "    # Display dropdown for city selection\n",
        "    print(\"\\nüèôÔ∏è Select a city to analyze:\")\n",
        "    for i, city in enumerate(city_options):\n",
        "        print(f\"{i}. {city}\")\n",
        "\n",
        "    # Get city selection\n",
        "    while True:\n",
        "        try:\n",
        "            city_idx = int(input(\"\\nEnter city number: \"))\n",
        "            if 0 <= city_idx < len(city_options):\n",
        "                selected_city = city_options[city_idx]\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid selection. Please try again.\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a number.\")\n",
        "\n",
        "    # Filter sites by selected city or use all\n",
        "    if selected_city == \"All Cities\":\n",
        "        sites_to_show = site_data\n",
        "        print(\"\\nüìç Select specific sites (or use demo sites):\")\n",
        "        print(\"d. Use first 5 sites for demo (recommended)\")\n",
        "        print(\"a. Use all sites (may be slow)\")\n",
        "\n",
        "        # Get site selection for all cities\n",
        "        selection = input(\"\\nEnter choice (d/a or comma-separated numbers): \")\n",
        "\n",
        "        if selection.lower() == 'd':\n",
        "            # Use first 5 sites for demo\n",
        "            selected_sites = site_data[:5]\n",
        "        elif selection.lower() == 'a':\n",
        "            # Use all sites\n",
        "            selected_sites = site_data\n",
        "        else:\n",
        "            # Try to parse comma-separated numbers\n",
        "            try:\n",
        "                # Show first 20 sites for selection\n",
        "                display_limit = min(20, len(site_data))\n",
        "                print(\"\\nAvailable sites:\")\n",
        "                for i in range(display_limit):\n",
        "                    site = site_data[i]\n",
        "                    print(f\"{i}. {site.get('name', 'Unknown')} ({site['id']}) - {site.get('city', 'Unknown')}\")\n",
        "\n",
        "                indices = [int(idx.strip()) for idx in selection.split(',')]\n",
        "                selected_sites = [site_data[idx] for idx in indices if 0 <= idx < len(site_data)]\n",
        "            except ValueError:\n",
        "                print(\"Invalid selection, using first 5 sites for demo.\")\n",
        "                selected_sites = site_data[:5]\n",
        "    else:\n",
        "        # Filter sites by selected city\n",
        "        sites_to_show = [site for site in site_data if site.get('city', 'Unknown') == selected_city]\n",
        "\n",
        "        print(f\"\\nüìç Select sites in {selected_city}:\")\n",
        "        for i, site in enumerate(sites_to_show):\n",
        "            print(f\"{i}. {site.get('name', 'Unknown')} ({site['id']})\")\n",
        "\n",
        "        print(\"a. Use all sites in this city\")\n",
        "\n",
        "        # Get site selection for specific city\n",
        "        selection = input(\"\\nEnter choice (a or comma-separated numbers): \")\n",
        "\n",
        "        if selection.lower() == 'a':\n",
        "            # Use all sites in this city\n",
        "            selected_sites = sites_to_show\n",
        "        else:\n",
        "            # Try to parse comma-separated numbers\n",
        "            try:\n",
        "                indices = [int(idx.strip()) for idx in selection.split(',')]\n",
        "                selected_sites = [sites_to_show[idx] for idx in indices if 0 <= idx < len(sites_to_show)]\n",
        "            except ValueError:\n",
        "                print(\"Invalid selection, using all sites in the city.\")\n",
        "                selected_sites = sites_to_show\n",
        "\n",
        "    # Show selected sites\n",
        "    print(f\"\\n‚úÖ Selected {len(selected_sites)} sites for analysis:\")\n",
        "    for site in selected_sites:\n",
        "        print(f\"  - {site.get('name', 'Unknown')} ({site['id']})\")\n",
        "\n",
        "    return selected_sites\n",
        "\n",
        "# Configure email settings\n",
        "def ask_supervisor_email():\n",
        "    \"\"\"Ask for supervisor's email\"\"\"\n",
        "    supervisor_email = input(\"\\nEnter supervisor's email: \").strip()\n",
        "    if supervisor_email and '@' in supervisor_email:\n",
        "        return supervisor_email\n",
        "    else:\n",
        "        # If invalid, use the default\n",
        "        print(f\"‚ö†Ô∏è Invalid email format. Using default recipient: {EMAIL_RECIPIENT}\")\n",
        "        return EMAIL_RECIPIENT\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"üöÄ Starting Air Quality Anomaly Detection System\")\n",
        "\n",
        "    # Check if files exist in Google Drive\n",
        "    files_exist = check_model_files()\n",
        "    if not files_exist:\n",
        "        print(\"‚ùå Missing required files in Google Drive. Please check the paths.\")\n",
        "        return None, None\n",
        "\n",
        "    # Load model and data from Google Drive\n",
        "    model, scaler, site_data = load_model_and_data()\n",
        "    if not all([model, scaler, site_data]):\n",
        "        print(\"‚ùå Failed to load model, scaler, or site data from Google Drive.\")\n",
        "        return None, None\n",
        "\n",
        "    # Ask user if they want to select specific sites\n",
        "    while True:\n",
        "        select_sites = input(\"\\nWould you like to select specific sites to analyze? (y/n): \")\n",
        "        if select_sites.lower() in ['y', 'yes', 'n', 'no']:\n",
        "            break\n",
        "        print(\"Please enter 'y' or 'n'.\")\n",
        "\n",
        "    selected_sites = None\n",
        "    if select_sites.lower() in ['y', 'yes']:\n",
        "        selected_sites = select_sites_for_analysis(site_data)\n",
        "    else:\n",
        "        print(\"Using first 5 sites for demo.\")\n",
        "        selected_sites = site_data[:5]\n",
        "\n",
        "    # Run anomaly detection\n",
        "    print(\"\\nüîç Running Anomaly Detection Process\")\n",
        "    results, summary = run_anomaly_detection(model, scaler, site_data, collection_results, collection_summaries, selected_sites)\n",
        "\n",
        "    # Display summary\n",
        "    if summary:\n",
        "        print(\"\\n===== üìä ANOMALY DETECTION SUMMARY =====\")\n",
        "        print(f\"üìÖ Date: {summary['date']}\")\n",
        "        print(f\"üìä Total records analyzed: {summary['total_records']}\")\n",
        "        print(f\"‚ö†Ô∏è Total anomalies detected: {summary['total_anomalies']} ({summary['anomaly_percentage']}%)\")\n",
        "        print(f\"üî¥ High confidence anomalies: {summary['high_confidence_anomalies']}\")\n",
        "\n",
        "        print(\"\\nüè¢ Top Sites with Anomalies:\")\n",
        "        for site in summary['top_anomaly_sites']:\n",
        "            print(f\"  - {site['site_name']} ({site['site_id']}): {site['anomaly_count']} anomalies\")\n",
        "\n",
        "    # Ask if user wants to send report to supervisor\n",
        "    if results is not None and not results.empty and summary is not None:\n",
        "        send_to_supervisor = input(\"\\nDo you want to send the report to your supervisor? (y/n): \")\n",
        "        if send_to_supervisor.lower() in ['y', 'yes']:\n",
        "            # Get supervisor email\n",
        "            print(\"\\nüìß Email Report Configuration\")\n",
        "            supervisor_email = ask_supervisor_email()\n",
        "\n",
        "            # Send email report\n",
        "            send_email_report(results, summary, supervisor_email)\n",
        "\n",
        "    print(\"\\n‚úÖ Anomaly Detection Process Complete!\")\n",
        "    return results, summary\n",
        "\n",
        "# Run the system\n",
        "if __name__ == \"__main__\":\n",
        "    # Suppress tensorflow warnings\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "    # Run main function\n",
        "    results, summary = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM7c2o5ZEaQT",
        "outputId": "13904e4b-f20b-41b8-b759-985281693b6a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Connected successfully to MongoDB Atlas!\n",
            "üöÄ Starting Air Quality Anomaly Detection System\n",
            "‚úÖ ensemble_model.keras found at /content/drive/MyDrive/WCE/ensemble_model/ensemble_model.keras\n",
            "‚úÖ ensemble_scaler.joblib found at /content/drive/MyDrive/WCE/ensemble_model/ensemble_scaler.joblib\n",
            "‚úÖ site_ids.json found at /content/drive/MyDrive/WCE/site_ids.json\n",
            "‚úÖ Ensemble model loaded successfully\n",
            "‚úÖ Scaler loaded successfully\n",
            "‚úÖ Loaded 95 sites\n",
            "\n",
            "Would you like to select specific sites to analyze? (y/n): y\n",
            "\n",
            "üèôÔ∏è Select a city to analyze:\n",
            "0. All Cities\n",
            "1. Bengaluru\n",
            "2. Delhi\n",
            "3. Hyderabad\n",
            "4. Kolkata\n",
            "5. Mumbai\n",
            "6. Unknown\n",
            "\n",
            "Enter city number: 3\n",
            "\n",
            "üìç Select sites in Hyderabad:\n",
            "0. Bollaram Industrial Area, Hyderabad - TSPCB (site_199)\n",
            "1. ICRISAT Patancheru, Hyderabad - TSPCB (site_251)\n",
            "2. Central University, Hyderabad - TSPCB (site_262)\n",
            "3. IDA Pashamylaram, Hyderabad - TSPCB (site_275)\n",
            "4. Sanathnagar, Hyderabad - TSPCB (site_294)\n",
            "5. Zoo Park, Hyderabad - TSPCB (site_298)\n",
            "6. New Malakpet, Hyderabad - TSPCB (site_5595)\n",
            "7. ECIL Kapra, Hyderabad - TSPCB (site_5596)\n",
            "8. IITH Kandi, Hyderabad - TSPCB (site_5597)\n",
            "9. Somajiguda, Hyderabad - TSPCB (site_5598)\n",
            "10. Kompally Municipal Office, Hyderabad - TSPCB (site_5599)\n",
            "11. Nacharam_TSIIC IALA, Hyderabad - TSPCB (site_5600)\n",
            "12. Ramachandrapuram, Hyderabad - TSPCB (site_5602)\n",
            "13. Kokapet, Hyderabad - TSPCB (site_5604)\n",
            "a. Use all sites in this city\n",
            "\n",
            "Enter choice (a or comma-separated numbers): 3\n",
            "\n",
            "‚úÖ Selected 1 sites for analysis:\n",
            "  - IDA Pashamylaram, Hyderabad - TSPCB (site_275)\n",
            "\n",
            "üîç Running Anomaly Detection Process\n",
            "üîç Analyzing data from 2025-03-14T05:01 to 2025-03-15T05:01 (last 24 hours)\n",
            "üìä Processing 1 selected sites\n",
            "\n",
            "üè¢ Processing site site_275 (IDA Pashamylaram, Hyderabad - TSPCB, Hyderabad)\n",
            "üìä Fetching data for site site_275 from 2025-03-14T05:01 to 2025-03-15T05:01\n",
            "‚ö†Ô∏è Could not retrieve API data for site site_275\n",
            "‚ö†Ô∏è Creating sample data for demonstration\n",
            "üîç Detected 28 anomalies out of 96 records\n",
            "‚úÖ Processed site site_275 with 96 records\n",
            "\n",
            "üìä Total records analyzed: 96\n",
            "üîç Total anomalies detected: 28\n",
            "‚úÖ Saved 96 detection results to MongoDB\n",
            "‚úÖ Saved summary to MongoDB with ID: 67d509a9d1c4c3e6c2e051fc\n",
            "\n",
            "===== üìä ANOMALY DETECTION SUMMARY =====\n",
            "üìÖ Date: 2025-03-15\n",
            "üìä Total records analyzed: 96\n",
            "‚ö†Ô∏è Total anomalies detected: 28 (29.17%)\n",
            "üî¥ High confidence anomalies: 28\n",
            "\n",
            "üè¢ Top Sites with Anomalies:\n",
            "  - IDA Pashamylaram, Hyderabad - TSPCB (site_275): 28 anomalies\n",
            "\n",
            "Do you want to send the report to your supervisor? (y/n): y\n",
            "\n",
            "üìß Email Report Configuration\n",
            "\n",
            "Enter supervisor's email: piyush.pise23@vit.edu\n",
            "‚úÖ CSV attachment created with 28 anomaly records\n",
            "‚úÖ Email report sent successfully to piyush.pise23@vit.edu\n",
            "\n",
            "‚úÖ Anomaly Detection Process Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9BIsTSJFswD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}